{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNPTmsfv44/3lZdn7FWSKfW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tommasomncttn/NAS4CNN/blob/main/Implementation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Importing "
      ],
      "metadata": {
        "id": "_bmtpvl1n5sN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from sklearn import datasets\n",
        "from sklearn.datasets import load_digits\n",
        "from torch.utils.data import DataLoader, Dataset"
      ],
      "metadata": {
        "id": "yNWkDl4JnPWo"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset "
      ],
      "metadata": {
        "id": "T6i4WgSHoBE_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TensorizedDigits(Dataset):\n",
        "    \"\"\"Scikit-Learn Digits dataset.\"\"\"\n",
        "\n",
        "    def __init__(self, mode = \"train\", transforms = None, tensorized = True):\n",
        "        digits = load_digits()\n",
        "        if mode == \"train\":\n",
        "            self.data = digits.data[:1000].astype(np.float32)\n",
        "            self.targets = digits.target[:1000]\n",
        "        elif mode == \"val\":\n",
        "            self.data = digits.data[1000:1350].astype(np.float32)\n",
        "            self.targets = digits.target[1000:1350]\n",
        "        else:\n",
        "            self.data = digits.data[1350:].astype(np.float32)\n",
        "            self.targets = digits.target[1350:]\n",
        "\n",
        "        self.transforms = transforms\n",
        "\n",
        "        if tensorized:\n",
        "          self.transforms = TensorizedDigits.tensorization_transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sample_x = self.data[idx]\n",
        "        sample_y = self.targets[idx]\n",
        "        \n",
        "        if True:\n",
        "          sample_x, sample_y = self.transforms(sample_x, sample_y)\n",
        "        \n",
        "\n",
        "        return (sample_x, sample_y)\n",
        "\n",
        "    @staticmethod\n",
        "    def tensorization_transform(x, y):\n",
        "        \n",
        "        # reshape to get a valid input for a CNN\n",
        "        sample_x = x.reshape(1, 8, 8)\n",
        "        sample_y = y\n",
        "\n",
        "        # transform it to torch tensor to move them to cuda\n",
        "        if torch.cuda.is_available():\n",
        "\n",
        "          sample_x = torch.from_numpy(sample_x).to(\"cuda\")\n",
        "          sample_y = np.array(y)\n",
        "          sample_y = torch.from_numpy(sample_y).to(\"cuda\")\n",
        "\n",
        "\n",
        "        return sample_x, sample_y\n",
        "    \n",
        "    def visualize_datapoint(self, idx):\n",
        "\n",
        "      x,y = self.__getitem__( idx)\n",
        "      plt.imshow(x[0].cpu(), cmap=\"gray\")\n",
        "      plt.axis(\"off\")\n",
        "      plt.show()\n"
      ],
      "metadata": {
        "id": "tTaJR9yuoAnc"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Module"
      ],
      "metadata": {
        "id": "gvKE7U34B_pL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiConfigCNN(nn.Module):\n",
        "    '''Conv2d → f(.) → Pooling → Flatten → Linear 1 → f(.) → Linear 2 → Softmax\n",
        "    '''\n",
        "    def __init__(self, cnn_i_N = 1, cnn_o_N = 8, cnn_k_size = 3, stride = 1, padding = 1, pool_k_size = 2, fnn_o_N = 10):\n",
        "        super(MultiConfigCNN, self).__init__()\n",
        "\n",
        "        self.cnn_i_N = cnn_i_N\n",
        "        self.cnn_o_N = cnn_o_N\n",
        "        self.cnn_k_size = cnn_k_size\n",
        "        self.stride = stride\n",
        "        self.padding = padding\n",
        "        self.pool_k_size = pool_k_size\n",
        "        self.fnn_o_N = fnn_o_N\n",
        "        \n",
        "        self.cnn =  nn.Conv2d(in_channels = cnn_i_N, out_channels = cnn_o_N, kernel_size = cnn_k_size, stride = stride, padding = padding)\n",
        "        self.activation1 = nn.ReLU() # or sigmoid, tanh, softplus, elu\n",
        "        self.pool = nn.MaxPool2d(kernel_size = pool_k_size) # or avg pool\n",
        "        self.flatten = nn.Flatten()\n",
        "\n",
        "        self.linear1 = nn.Linear(in_features = self.compute_input_2_linear(), out_features = fnn_o_N)\n",
        "        self.activation2 = nn.ReLU() # or sigmoid, tanh, softplus, elu\n",
        "        self.linear2 = nn.Linear(in_features = fnn_o_N, out_features = 10)\n",
        "        self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "        self.nll = nn.NLLLoss(reduction=\"none\") \n",
        "    \n",
        "    def compute_input_2_linear(self):\n",
        "\n",
        "        # computing after convolution => [(W-K+2P)/S]+1\n",
        "        after_cnn_channels = self.cnn_o_N\n",
        "        after_cnn_height = after_cnn_width = ((8 - self.cnn_k_size + 2 * self.padding) / self.stride) + 1\n",
        "\n",
        "        # computing after pooling => fixed values stride=kernel_dimension, padding=0, dilation=1 => formula at end of https://pytorch.org/docs/stable/generated/torch.nn.MaxPool2d.html\n",
        "        after_pool_height = after_pool_width = ((after_cnn_height - self.pool_k_size) / self.pool_k_size ) + 1\n",
        "\n",
        "        # computing after flattening \n",
        "\n",
        "        return int(after_pool_height * after_pool_width * after_cnn_channels)\n",
        "\n",
        "    def classify(self, log_prob):\n",
        "        \n",
        "        y_pred = torch.argmax(log_prob, dim = 1).long()        \n",
        "        return y_pred\n",
        "\n",
        "    def forward(self, x):\n",
        "        \n",
        "        x = self.cnn(x)\n",
        "        x = self.activation1(x)\n",
        "        x = self.pool(x)\n",
        "        x = self.flatten(x)\n",
        "        x = self.linear1(x)\n",
        "        x = self.activation2(x)\n",
        "        x = self.linear2(x)\n",
        "        log_prob = self.softmax(x)\n",
        "\n",
        "        return log_prob\n",
        "\n",
        "\n",
        "    def compute_loss(self, log_prob, y, reduction=\"avg\"):\n",
        "\n",
        "        loss = self.nll(log_prob, y)\n",
        "\n",
        "        if reduction == \"sum\":\n",
        "            return loss.sum()\n",
        "\n",
        "        else:\n",
        "            return loss.mean()"
      ],
      "metadata": {
        "id": "Vp1stkTGB_Pu"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Inference "
      ],
      "metadata": {
        "id": "p6Mp6NDjB6f8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize training, validation and test sets.\n",
        "train_data = TensorizedDigits(mode=\"train\")\n",
        "val_data = TensorizedDigits(mode=\"val\")\n",
        "test_data = TensorizedDigits(mode=\"test\")\n",
        "\n",
        "# Initialize data loaders.\n",
        "training_loader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
        "val_loader = DataLoader(val_data, batch_size=64, shuffle=False)\n",
        "test_loader = DataLoader(test_data, batch_size=64, shuffle=False)"
      ],
      "metadata": {
        "id": "XGJo9-A8BiPs"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cnn = MultiConfigCNN().to(\"cuda\")"
      ],
      "metadata": {
        "id": "AJN412WmNC8L"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for (x,y) in training_loader:\n",
        "  print(x.shape)\n",
        "  print(y.shape)\n",
        "  log_prob = cnn(x)\n",
        "  prediction  = cnn.classify(log_prob)\n",
        "  loss = cnn.compute_loss(log_prob,y)\n",
        "  print(log_prob)\n",
        "  print(prediction)\n",
        "  print(loss)\n",
        "  break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RmUwkh0-NMmd",
        "outputId": "d85166d4-76f8-4854-b5c1-b22178971c50"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([64, 1, 8, 8])\n",
            "torch.Size([64])\n",
            "tensor([[-2.9268, -2.3019, -1.5868, -2.1705, -2.4657, -2.8130, -2.6202, -2.1974,\n",
            "         -1.8483, -3.1867],\n",
            "        [-2.7134, -1.8405, -1.9471, -2.3994, -2.3205, -2.7197, -3.2732, -1.7785,\n",
            "         -1.9183, -3.7402],\n",
            "        [-2.6657, -2.4396, -1.4572, -2.6660, -2.1420, -2.8865, -2.9269, -2.0369,\n",
            "         -1.7373, -4.8717],\n",
            "        [-3.5039, -2.2260, -1.8618, -2.1771, -1.7625, -2.7575, -3.0667, -2.0522,\n",
            "         -1.7615, -4.4808],\n",
            "        [-3.4331, -2.4226, -1.6819, -2.2764, -2.3049, -2.8132, -2.8807, -1.9556,\n",
            "         -1.5514, -3.8612],\n",
            "        [-3.5270, -2.2432, -1.9177, -2.2888, -1.7612, -2.7167, -3.0654, -1.9051,\n",
            "         -1.7638, -4.4750],\n",
            "        [-3.0204, -2.8771, -1.4700, -2.7255, -2.3917, -2.6758, -2.4756, -1.9670,\n",
            "         -1.7086, -3.3798],\n",
            "        [-3.2406, -3.0188, -1.3734, -2.4237, -2.1642, -2.8597, -2.2991, -2.4220,\n",
            "         -1.7006, -3.6354],\n",
            "        [-3.4236, -2.3070, -1.8139, -2.3695, -1.8955, -2.7605, -3.0689, -1.8840,\n",
            "         -1.6662, -4.5656],\n",
            "        [-3.1570, -2.5569, -1.7558, -2.1790, -2.2121, -2.7213, -2.8699, -2.3812,\n",
            "         -1.4053, -3.7182],\n",
            "        [-4.0809, -2.9775, -1.7744, -2.5662, -1.9233, -2.6154, -2.7966, -1.8393,\n",
            "         -1.4833, -3.9293],\n",
            "        [-3.9115, -2.5386, -1.6937, -2.4783, -1.9208, -2.8446, -3.0569, -1.7314,\n",
            "         -1.6288, -4.7776],\n",
            "        [-3.7904, -1.8631, -1.8939, -2.0057, -1.9159, -3.0360, -3.4756, -1.9425,\n",
            "         -1.8244, -5.0634],\n",
            "        [-3.6904, -2.7429, -1.4015, -1.9993, -2.1847, -3.0075, -2.3424, -2.5121,\n",
            "         -1.7785, -3.8644],\n",
            "        [-3.4463, -2.4943, -1.8464, -2.2100, -1.7213, -2.8353, -2.7947, -2.1273,\n",
            "         -1.6699, -4.4238],\n",
            "        [-2.8657, -2.5426, -1.6350, -2.5094, -2.6236, -2.6896, -2.6645, -1.9800,\n",
            "         -1.6672, -2.9720],\n",
            "        [-3.2520, -2.5221, -1.6471, -2.5209, -2.0051, -2.7322, -2.8507, -1.9052,\n",
            "         -1.6686, -4.3282],\n",
            "        [-2.7063, -2.9113, -1.2573, -2.8351, -2.2476, -2.9062, -2.5679, -2.2075,\n",
            "         -1.7194, -4.6555],\n",
            "        [-2.8546, -2.5783, -1.3396, -2.2259, -2.8331, -2.9579, -2.4792, -2.4344,\n",
            "         -1.8462, -2.8727],\n",
            "        [-3.4074, -3.1709, -1.3619, -2.3303, -2.3823, -3.1128, -2.5191, -2.6106,\n",
            "         -1.4008, -3.3749],\n",
            "        [-2.9717, -1.9207, -2.0126, -2.3974, -2.1205, -2.6708, -3.2981, -1.7403,\n",
            "         -1.8527, -3.9488],\n",
            "        [-2.7942, -2.5907, -1.5945, -2.3405, -2.2698, -2.6704, -2.3841, -2.3104,\n",
            "         -1.8550, -3.1193],\n",
            "        [-3.0787, -3.0112, -1.3752, -2.5006, -2.3873, -2.7855, -2.2164, -2.3552,\n",
            "         -1.7329, -3.3327],\n",
            "        [-3.2881, -2.8958, -1.6400, -2.5569, -2.2975, -2.6732, -2.5819, -2.0659,\n",
            "         -1.4410, -3.5990],\n",
            "        [-3.6619, -2.3561, -1.7936, -2.1284, -1.9191, -2.8761, -3.0157, -2.0368,\n",
            "         -1.6207, -4.2806],\n",
            "        [-3.6970, -2.0931, -1.7778, -2.1827, -1.9896, -2.9352, -3.2292, -1.8391,\n",
            "         -1.7546, -4.7383],\n",
            "        [-3.1492, -3.0186, -1.5427, -2.6254, -2.6011, -2.6486, -2.2937, -2.1102,\n",
            "         -1.7056, -2.6127],\n",
            "        [-3.4914, -3.3352, -1.4007, -2.5258, -2.7404, -2.8147, -2.0426, -2.3906,\n",
            "         -1.7125, -2.5089],\n",
            "        [-3.6492, -2.6601, -1.4148, -2.1080, -2.0884, -2.9854, -2.5018, -2.3436,\n",
            "         -1.7569, -4.2194],\n",
            "        [-3.4164, -2.7579, -1.8208, -2.5584, -1.7754, -2.4960, -2.9000, -2.0236,\n",
            "         -1.5817, -3.9246],\n",
            "        [-2.7665, -2.8704, -1.5062, -2.5398, -3.0471, -2.8308, -2.4536, -2.2942,\n",
            "         -1.5455, -2.6130],\n",
            "        [-3.3164, -2.3479, -1.8069, -2.1699, -2.3310, -2.8461, -3.0008, -2.1224,\n",
            "         -1.3965, -4.0407],\n",
            "        [-3.8842, -3.0022, -1.6868, -2.7325, -1.9011, -2.6276, -2.9532, -1.8200,\n",
            "         -1.4701, -4.2820],\n",
            "        [-3.4399, -3.0462, -1.4439, -2.4715, -2.3997, -2.8163, -2.3843, -2.2348,\n",
            "         -1.5037, -3.5796],\n",
            "        [-2.8996, -2.1195, -1.9274, -2.4387, -2.1956, -2.5989, -3.0369, -1.7968,\n",
            "         -1.8147, -3.5158],\n",
            "        [-3.7729, -2.5574, -1.6092, -2.1585, -2.1253, -2.8423, -2.6763, -2.0631,\n",
            "         -1.6626, -3.8751],\n",
            "        [-3.3353, -3.3772, -1.3137, -2.4020, -2.6340, -2.9404, -1.9906, -2.7235,\n",
            "         -1.6112, -3.1156],\n",
            "        [-3.1036, -2.5729, -1.6666, -2.1632, -2.4246, -3.0706, -2.8223, -2.3657,\n",
            "         -1.2991, -4.2743],\n",
            "        [-3.1107, -2.5381, -1.8628, -2.4156, -2.1188, -2.5198, -2.6196, -1.9957,\n",
            "         -1.7275, -3.1072],\n",
            "        [-3.4800, -2.6879, -1.6578, -2.2065, -2.1192, -2.6973, -2.4384, -2.2161,\n",
            "         -1.7090, -3.3345],\n",
            "        [-4.0429, -3.0673, -1.5697, -2.7914, -2.2656, -2.7634, -2.7663, -1.6364,\n",
            "         -1.4963, -4.0127],\n",
            "        [-2.9890, -3.0642, -1.5670, -2.6046, -2.8160, -2.6992, -2.3028, -2.2640,\n",
            "         -1.5798, -2.4849],\n",
            "        [-3.0996, -2.4460, -1.3873, -2.1456, -2.6043, -2.9898, -2.5542, -2.2876,\n",
            "         -1.7955, -3.4429],\n",
            "        [-2.8307, -3.1424, -1.5361, -2.8094, -2.6222, -2.5903, -2.1971, -2.1866,\n",
            "         -1.7787, -2.4955],\n",
            "        [-2.9469, -2.8571, -1.3481, -2.3399, -2.8604, -2.9725, -2.3592, -2.4722,\n",
            "         -1.5952, -3.1283],\n",
            "        [-3.0101, -3.1814, -1.5669, -2.5799, -2.8471, -2.7158, -2.1911, -2.4097,\n",
            "         -1.5918, -2.3501],\n",
            "        [-3.1505, -3.4817, -1.2471, -2.8413, -2.6210, -2.8143, -2.0139, -2.4494,\n",
            "         -1.7411, -2.9390],\n",
            "        [-3.5209, -2.2992, -1.6798, -2.1260, -1.9777, -2.8549, -2.8505, -2.0791,\n",
            "         -1.7631, -4.2656],\n",
            "        [-3.0773, -1.8730, -1.9446, -2.3284, -2.1675, -2.7730, -3.3315, -1.7443,\n",
            "         -1.8558, -4.1350],\n",
            "        [-3.6692, -3.5056, -1.4724, -2.8476, -2.5195, -2.6797, -2.2244, -2.0560,\n",
            "         -1.4958, -3.0393],\n",
            "        [-3.2278, -2.2144, -1.6129, -2.1702, -2.1576, -2.8728, -2.8551, -2.0647,\n",
            "         -1.8065, -4.0811],\n",
            "        [-3.6621, -2.2311, -1.7840, -2.2536, -1.9541, -2.8384, -3.1010, -1.8406,\n",
            "         -1.7201, -4.5136],\n",
            "        [-2.6577, -2.5972, -1.7229, -2.8397, -2.7232, -2.5672, -2.7408, -1.7572,\n",
            "         -1.7767, -2.6626],\n",
            "        [-2.5159, -2.0713, -1.8886, -2.4231, -2.3033, -2.6567, -3.0435, -1.9848,\n",
            "         -1.7607, -3.6441],\n",
            "        [-3.4672, -3.0041, -1.6539, -2.5056, -2.2865, -2.5888, -2.3268, -2.0932,\n",
            "         -1.6423, -2.8985],\n",
            "        [-2.5527, -2.0363, -1.9117, -2.5682, -2.5154, -2.6519, -3.1752, -1.7510,\n",
            "         -1.7919, -3.3886],\n",
            "        [-3.0327, -2.7965, -1.6527, -2.4546, -2.2124, -2.6987, -2.5741, -2.2658,\n",
            "         -1.4498, -3.8143],\n",
            "        [-2.6762, -3.2329, -1.2596, -2.6772, -2.2491, -2.8444, -2.0456, -2.6805,\n",
            "         -1.8659, -3.7748],\n",
            "        [-3.5605, -2.1410, -1.9182, -2.4659, -2.0768, -2.7707, -3.3451, -1.5778,\n",
            "         -1.7118, -4.4167],\n",
            "        [-3.5164, -2.9960, -1.5084, -2.2528, -2.2759, -2.7632, -2.1699, -2.4131,\n",
            "         -1.7216, -3.0872],\n",
            "        [-3.6164, -2.1301, -1.7737, -2.1228, -1.8585, -2.9449, -3.1606, -2.0046,\n",
            "         -1.7667, -4.7830],\n",
            "        [-3.3984, -2.5368, -1.4700, -2.1187, -2.0442, -2.9881, -2.6519, -2.3531,\n",
            "         -1.7342, -4.1770],\n",
            "        [-3.2164, -2.8157, -1.5955, -2.2621, -2.3196, -2.6893, -2.2435, -2.3701,\n",
            "         -1.7724, -2.8583],\n",
            "        [-3.0085, -3.3537, -1.2856, -2.7411, -2.7521, -2.8152, -2.0168, -2.4216,\n",
            "         -1.8067, -2.7341]], device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n",
            "tensor([2, 7, 2, 8, 8, 4, 2, 2, 8, 8, 8, 8, 8, 2, 8, 2, 2, 2, 2, 2, 7, 2, 2, 8,\n",
            "        8, 8, 2, 2, 2, 8, 2, 8, 8, 2, 7, 2, 2, 8, 8, 2, 8, 2, 2, 2, 2, 2, 2, 2,\n",
            "        7, 2, 2, 8, 2, 8, 8, 7, 8, 2, 7, 2, 8, 2, 2, 2], device='cuda:0')\n",
            "tensor(2.6888, device='cuda:0', grad_fn=<MeanBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iEZnW3YPN1CC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}